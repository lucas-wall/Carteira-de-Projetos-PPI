{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><i>Importação de Bibliotecas</i><h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><i>Configuração de Sessão</i></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo o User-Agent para a sessão\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36 OPR/109.0.0.0'\n",
    "}\n",
    "\n",
    "# OPERA = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36 OPR/109.0.0.0'\n",
    "# EDGE = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36 Edg/126.0.0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL para a página de login\n",
    "login_url = 'https://ppi.gov.br/acesso/'\n",
    "# Configuração inicial da sessão de requisições com login\n",
    "session = requests.Session()\n",
    "session.headers.update(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acessar a página de login para obter campos ocultos e CSRF\n",
    "response = session.get(login_url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "# Obter campos ocultos para incluir na solicitação de login\n",
    "hidden_inputs = {tag['name']: tag.get('value', '') for tag in soup.find_all('input', type='hidden')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    'log': 'Lucas',  # Nome de usuário\n",
    "    'pwd': 'PrPPI2024',  # Senha\n",
    "    **hidden_inputs  # Incluir campos ocultos\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login efetuado com sucesso.\n"
     ]
    }
   ],
   "source": [
    "# Tentar fazer o login\n",
    "login_response = session.post(login_url, data=payload)\n",
    "\n",
    "# Verificar se o login foi bem-sucedido\n",
    "if \"Perfil\" in login_response.text or \"perfil\" in login_response.url:\n",
    "    print(\"Login efetuado com sucesso.\")\n",
    "else:\n",
    "    print(\"Falha no login. Verifique credenciais e URL.\")\n",
    "    print(\"Detalhes da resposta:\", login_response.text[:1000])  # Para diagnóstico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><i>Definição de Funções</i></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_or_none(soup, selector, attr_name=None, attr_value=None):\n",
    "    # Encontrar o elemento baseado no seletor e atributos\n",
    "    if attr_name and attr_value:\n",
    "        element = soup.find(selector, {attr_name: attr_value})\n",
    "    else:\n",
    "        element = soup.find(selector)\n",
    "    \n",
    "    # Retornar o valor do elemento se encontrado, senão None\n",
    "    return element['value'] if element and 'value' in element.attrs else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_checked_checkboxes(soup, name):\n",
    "    checked_boxes = soup.find_all('input', {'type': 'checkbox', 'name': name})\n",
    "    checked_labels = []\n",
    "    for box in checked_boxes:\n",
    "        if box.has_attr('checked'):\n",
    "            # Encontrar o label associado ao checkbox marcado\n",
    "            label = box.find_parent('label')\n",
    "            if label:\n",
    "                checked_labels.append(label.get_text(strip=True))\n",
    "    return checked_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para obter opções selecionadas de um select de forma segura\n",
    "def get_selected_options_safe(soup, select_id):\n",
    "    select = soup.find('select', {'id': select_id})\n",
    "    if select is not None:\n",
    "        return [(option['value'], option.text.strip()) for option in select.find_all('option', selected=True)]\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_select_option(soup, field_name):\n",
    "    select_element = soup.find('select', {'name': field_name})\n",
    "    if select_element:\n",
    "        selected_option = select_element.find('option', selected=True)\n",
    "        if selected_option:\n",
    "            return selected_option.get_text(strip=True)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_etapas(soup):\n",
    "    etapas = []\n",
    "    for i in range(6):  # Assumindo que há 6 etapas como discutido\n",
    "        etapa_info = {}\n",
    "        radio_checked = soup.find('input', {'name': f'acf[field_6310f4ec9b055][row-{i}][field_6311023434b2e]', 'checked': True})\n",
    "        if radio_checked:\n",
    "            etapa_info['etapa'] = radio_checked.find_next_sibling('span').text.strip()\n",
    "        \n",
    "        # Extrair o status selecionado\n",
    "        status_select = soup.find('select', {'id': f'acf-field_6310f4ec9b055-row-{i}-field_631112a34b7f6'})\n",
    "        if status_select:\n",
    "            selected_option = status_select.find('option', selected=lambda x: x is not None)\n",
    "            etapa_info['status'] = selected_option.text.strip() if selected_option else \"\"\n",
    "        else:\n",
    "            etapa_info['status'] = \"\"\n",
    "\n",
    "        # Extrair datas de início e fim\n",
    "        start_date_input = soup.find('input', {'id': f'acf-field_6310f4ec9b055-row-{i}-field_6311094d8fae5'})\n",
    "        etapa_info['data_inicio'] = start_date_input['value'] if start_date_input else \"\"\n",
    "\n",
    "        end_date_input = soup.find('input', {'id': f'acf-field_6310f4ec9b055-row-{i}-field_631109a88fae7'})\n",
    "        etapa_info['data_fim'] = end_date_input['value'] if end_date_input else \"\"\n",
    "\n",
    "        etapas.append(etapa_info)\n",
    "    return etapas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para extrair reuniões\n",
    "def get_reunions(soup):\n",
    "    reunions = []\n",
    "    reunion_fields = soup.find_all('td', {'class': 'acf-field-text'})\n",
    "    for field in reunion_fields:\n",
    "        input_element = field.find('input', {'type': 'text'})\n",
    "        if input_element and input_element.has_attr('value'):\n",
    "            reunions.append(input_element['value'])\n",
    "    return reunions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para Extrair a Data de Qualificação\n",
    "def get_date_field(soup, field_name):\n",
    "    date_element = soup.find('input', {'name': field_name})\n",
    "    if date_element and 'value' in date_element.attrs:\n",
    "        return date_element['value']\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para Extrair Ano do Leilão\n",
    "def get_number_field(soup, field_name):\n",
    "    number_element = soup.find('input', {'name': field_name})\n",
    "    if number_element and 'value' in number_element.attrs:\n",
    "        return number_element['value']\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para extrair o link do projeto\n",
    "def get_project_link(soup):\n",
    "    link_element = soup.find('span', {'id': 'sample-permalink'}).find('a')\n",
    "    if link_element and 'href' in link_element.attrs:\n",
    "        return link_element['href']\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><i>Processamento de URLs</i></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_project_links(base_url, max_pages=100):\n",
    "    project_urls = []\n",
    "    last_valid_url = base_url\n",
    "    for page in tqdm(range(1, max_pages + 1), desc=\"Extraindo links\"):\n",
    "        page_url = f\"{base_url}&paged={page}\"\n",
    "        response = session.get(page_url, allow_redirects=True)\n",
    "        \n",
    "        # Verifica se a página foi redirecionada para a última URL válida\n",
    "        if response.url == last_valid_url:\n",
    "            print(\"Nenhuma página nova, parando a busca.\")\n",
    "            break\n",
    "        else:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            links = soup.find_all('a', href=True)\n",
    "            for link in links:\n",
    "                if '/post.php?post=' in link['href'] and '&action=edit' in link['href']:\n",
    "                    full_url = urljoin(base_url, link['href'])\n",
    "                    if full_url not in project_urls:\n",
    "                        project_urls.append(full_url)\n",
    "            last_valid_url = response.url  # Atualiza o último URL válido após processar a página\n",
    "    return project_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraindo links:  32%|███▏      | 32/100 [01:24<02:58,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nenhuma página nova, parando a busca.\n",
      "Total de links coletados: 613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Iniciar a extração\n",
    "base_url = 'https://ppi.gov.br/wp-admin/edit.php?post_status=publish&post_type=projetos'\n",
    "project_urls = fetch_project_links(base_url)\n",
    "print(f\"Total de links coletados: {len(project_urls)}\")\n",
    "\n",
    "# Salvar os links em um arquivo\n",
    "with open('project_urls.txt', 'w') as file:\n",
    "    for url in project_urls:\n",
    "        file.write(url + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><i>Extração dos Dados</i></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraindo dados dos projetos: 100%|██████████| 613/613 [16:52<00:00,  1.65s/it]\n"
     ]
    }
   ],
   "source": [
    "data_list = []  # Lista para armazenar os dados coletados# Processar cada URL do projeto\n",
    "\n",
    "for url in tqdm(project_urls, desc='Extraindo dados dos projetos'):\n",
    "    response = session.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Falha ao acessar {url}, Status Code: {response.status_code}\")\n",
    "        continue\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    title = soup.find('input', {'id': 'title'}).get('value')\n",
    "    description = soup.find('textarea', {'id': 'content'}).text\n",
    "    status = get_checked_checkboxes(soup, 'tax_input[status][]')\n",
    "    uf = get_checked_checkboxes(soup, 'tax_input[uf][]')\n",
    "    setores = get_checked_checkboxes(soup, 'tax_input[setores][]')\n",
    "    ods = get_checked_checkboxes(soup, 'tax_input[ods][]')\n",
    "    id_projeto = get_text_or_none(soup, 'input', 'name', 'acf[field_636a4fbba8b53]')\n",
    "    ministerio = get_text_or_none(soup, 'input', 'name', 'acf[field_636a91aa285db]')\n",
    "    secretaria = soup.find('input', {'id': 'acf-field_636a91df285dd'}).get('value')\n",
    "    resolucoes = get_selected_options_safe(soup, 'acf-field_6388d4eae3d87')\n",
    "    decretos = get_selected_options_safe(soup, 'acf-field_6388d5f03c024')\n",
    "    etapas = extract_etapas(soup)\n",
    "    reunions = get_reunions(soup)\n",
    "    data_qualificacao = get_date_field(soup, 'acf[field_636a8610ce9ae]')\n",
    "    modalidade_operacional = get_select_option(soup, 'acf[field_636a89fd5a3cf]')\n",
    "    detalhamento_modalidade_operacional = get_select_option(soup, 'acf[field_636a8a70b765c]')\n",
    "    capex = get_text_or_none(soup, 'input', 'name', 'acf[field_63231967a8b12]')\n",
    "    leilao = get_select_option(soup, 'acf[field_636aa61ecda34]')\n",
    "    ano_leilao = get_number_field(soup, 'acf[field_64bec474d7c03]')\n",
    "    projeto_ativo = get_select_option(soup, 'acf[field_636a98a70c7a4]')\n",
    "    permalink = get_project_link(soup)\n",
    "    \n",
    "    \n",
    "    project_data = {\n",
    "        'link': permalink,\n",
    "        'title': title,\n",
    "        'status': status,\n",
    "        'uf': uf,\n",
    "        'setores': setores,\n",
    "        'ods': ods,\n",
    "        'identificador_do_projeto': id_projeto,\n",
    "        'ministerio': ministerio,\n",
    "        'secretaria': secretaria,\n",
    "        'resolucoes': resolucoes,\n",
    "        'decretos': decretos,\n",
    "        'etapas': etapas,\n",
    "        'reuniao': reunions,\n",
    "        'data_qualificacao': data_qualificacao,\n",
    "        'modalidade_operacional': modalidade_operacional,\n",
    "        'detalhamento_modalidade_operacional': detalhamento_modalidade_operacional,\n",
    "        'capex': capex,\n",
    "        'leilao': leilao,\n",
    "        'ano_leilao': ano_leilao,\n",
    "        'projeto_ativo': projeto_ativo\n",
    "    }\n",
    "    \n",
    "    data_list.append(project_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><i>Salvamento dos Dados</i></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar os dados coletados em um arquivo JSON\n",
    "with open('project_data.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(data_list, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\PR1049457\\\\lucas.bruno\\\\Downloads\\\\Projects\\\\PPI.GOV\\\\Pandas\\\\project_data_copy.json'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fazer uma cópia do arquivo JSON diretamente para o caminho desejado\n",
    "shutil.copy('project_data.json', r'C:\\PR1049457\\lucas.bruno\\Downloads\\Projects\\PPI.GOV\\Pandas\\project_data_copy.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><i>SCRAP PPI.GOV - PROJETOS</i></h1>\n",
    "<h1><i>Importação de Bibliotecas</i></h1>\n",
    "<p>O script importa diversas bibliotecas necessárias para o processo de scraping e manipulação dos dados:</p>\n",
    "<pre>\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "from urllib.parse import urljoin\n",
    "</pre>\n",
    "<p>Essas bibliotecas são utilizadas para requisições HTTP, parsing de HTML, manipulação de JSON, monitoramento do progresso e operações em arquivos e URLs.</p>\n",
    "\n",
    "<h1><i>Configuração de Sessão</i></h1>\n",
    "<p>A seguir, o código configura a sessão de requisição HTTP:</p>\n",
    "<pre>\n",
    "# Definindo o User-Agent para a sessão\n",
    "headers = {\n",
    "'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36 OPR/109.0.0.0'\n",
    "}\n",
    "# URL para a página de login\n",
    "login_url = 'https://ppi.gov.br/acesso/'\n",
    "session = requests.Session()\n",
    "session.headers.update(headers)\n",
    "response = session.get(login_url)\n",
    "</pre>\n",
    "<p>Após definir o <code>User-Agent</code>, o script inicializa uma sessão e faz uma requisição para a página de login para buscar campos ocultos necessários para realizar o login.</p>\n",
    "\n",
    "<h1><i>Definição de Funções</i></h1>\n",
    "<p>O script define várias funções para extrair dados de forma eficiente da página HTML:</p>\n",
    "<ul>\n",
    "<li><b><i>get_text_or_none</i></b>: Extrai o valor de um campo HTML, caso exista.</li>\n",
    "<li><b><i>get_checked_checkboxes</i></b>: Extrai os valores de checkboxes marcados.</li>\n",
    "<li><b><i>get_selected_options_safe</i></b>: Extrai as opções selecionadas de um campo <code>select</code> de forma segura.</li>\n",
    "<li><b><i>get_select_option</i></b>: Extrai o valor selecionado de um campo <code>select</code>.</li>\n",
    "<li><b><i>extract_etapas</i></b>: Extrai as etapas de um projeto, considerando as entradas de tipo radio button.</li>\n",
    "<li><b><i>get_reunions</i></b>: Extrai os valores de reuniões cadastradas em campos de texto.</li>\n",
    "<li><b><i>get_date_field</i></b>: Extrai uma data de um campo específico.</li>\n",
    "<li><b><i>get_number_field</i></b>: Extrai um valor numérico de um campo.</li>\n",
    "<li><b><i>get_project_link</i></b>: Extrai o link permanente do projeto.</li>\n",
    "</ul>\n",
    "\n",
    "<h1><i>Processamento de URLs</i></h1>\n",
    "<p>O script então processa URLs de projetos a partir de uma página principal de listagem de projetos, coletando os links para cada projeto individualmente:</p>\n",
    "<pre>\n",
    "def fetch_project_links(base_url, max_pages=100):\n",
    "project_urls = []\n",
    "last_valid_url = base_url\n",
    "for page in tqdm(range(1, max_pages + 1), desc=\"Extraindo links\"):\n",
    "page_url = f\"{base_url}&paged={page}\"\n",
    "response = session.get(page_url, allow_redirects=True)\n",
    "if response.url == last_valid_url:\n",
    "print(\"Nenhuma página nova, parando a busca.\")\n",
    "break\n",
    "else:\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "links = soup.find_all('a', href=True)\n",
    "for link in links:\n",
    "if '/post.php?post=' in link['href'] and '&action=edit' in link['href']:\n",
    "    full_url = urljoin(base_url, link['href'])\n",
    "    if full_url not in project_urls:\n",
    "        project_urls.append(full_url)\n",
    "last_valid_url = response.url\n",
    "return project_urls\n",
    "</pre>\n",
    "<p>O script extrai os links dos projetos através de um loop nas páginas de resultados, verificando se há novos links e coletando-os.</p>\n",
    "\n",
    "<h1><i>Extração dos Dados</i></h1>\n",
    "<p>A coleta dos dados de cada projeto é feita por meio de requisições à URL de cada projeto individualmente:</p>\n",
    "<pre>\n",
    "data_list = []  # Lista para armazenar os dados coletados\n",
    "\n",
    "for url in tqdm(project_urls, desc='Extraindo dados dos projetos'):\n",
    "response = session.get(url)\n",
    "if response.status_code != 200:\n",
    "print(f\"Falha ao acessar {url}, Status Code: {response.status_code}\")\n",
    "continue\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "title = soup.find('input', {'id': 'title'}).get('value')\n",
    "description = soup.find('textarea', {'id': 'content'}).text\n",
    "status = get_checked_checkboxes(soup, 'tax_input[status][]')\n",
    "uf = get_checked_checkboxes(soup, 'tax_input[uf][]')\n",
    "...\n",
    "\n",
    "project_data = {\n",
    "'link': permalink,\n",
    "'title': title,\n",
    "'status': status,\n",
    "'uf': uf,\n",
    "'setores': setores,\n",
    "...\n",
    "}\n",
    "\n",
    "data_list.append(project_data)\n",
    "</pre>\n",
    "<p>Para cada URL de projeto, o script realiza uma requisição e utiliza as funções definidas anteriormente para extrair diversas informações sobre o projeto, como título, descrição, status, e etapas, e armazena os dados coletados em uma lista.</p>\n",
    "\n",
    "<h1><i>Salvamento dos Dados</i></h1>\n",
    "<p>Após a extração dos dados, o script salva as informações em um arquivo JSON:</p>\n",
    "<pre>\n",
    "with open('project_data.json', 'w', encoding='utf-8') as file:\n",
    "json.dump(data_list, file, ensure_ascii=False, indent=4)\n",
    "</pre>\n",
    "<p>Além disso, o script faz uma cópia do arquivo JSON:</p>\n",
    "<pre>\n",
    "shutil.copy('project_data.json', 'project_data_copy.json')\n",
    "</pre>\n",
    "<p>Esses arquivos JSON contêm todos os dados extraídos, sendo a cópia criada para fins de backup ou processamento adicional.</p>\n",
    "\n",
    "<h2><i>Conclusão</i></h2>\n",
    "<p>Este script realiza a coleta de dados estruturados de projetos de um site específico, utilizando técnicas de scraping e armazenamento seguro dos dados extraídos em um arquivo JSON. Ele também garante a integridade dos dados através de cópias de segurança.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
